{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "#seed( ) 用于指定随机数生成时所用算法开始的整数值。\n",
    "#1.如果使用相同的seed( )值，则每次生成的随即数都相同；\n",
    "#2.如果不设置这个值，则系统根据时间来自己选择这个值，此时每次生成的随机数因时间差异而不同。\n",
    "#3.设置的seed()值仅一次有效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_file = './data/stock_comments_seg.csv'\n",
    "data_path = './data'\n",
    "pos_corpus = 'positive.txt'\n",
    "neg_corpus = 'negative.txt'\n",
    "K_Best_Features = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    pos_file = os.path.join(data_path, pos_corpus)\n",
    "    neg_file = os.path.join(data_path, neg_corpus)\n",
    "\n",
    "    pos_sents = []\n",
    "    with open(pos_file, 'r', encoding='utf-8') as f:\n",
    "        for sent in f:\n",
    "            pos_sents.append(sent.replace('\\n', ''))\n",
    "\n",
    "    neg_sents = []\n",
    "    with open(neg_file, 'r', encoding='utf-8') as f:\n",
    "        for sent in f:\n",
    "            neg_sents.append(sent.replace('\\n', ''))\n",
    "\n",
    "    balance_len = min(len(pos_sents), len(neg_sents))\n",
    "\n",
    "    pos_df = pd.DataFrame(pos_sents, columns=['text'])\n",
    "    pos_df['polarity'] = 1\n",
    "    pos_df = pos_df[:balance_len]\n",
    "\n",
    "    neg_df = pd.DataFrame(neg_sents, columns=['text'])\n",
    "    neg_df['polarity'] = 0\n",
    "    neg_df = neg_df[:balance_len]\n",
    "\n",
    "    return pd.concat([pos_df, neg_df]).reset_index(drop=True)\n",
    "#    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_tokenized():\n",
    "    pos_file = os.path.join(data_path, pos_corpus)\n",
    "    neg_file = os.path.join(data_path, neg_corpus)\n",
    "\n",
    "    pos_sents = []\n",
    "    with open(pos_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokens = line.split(' ')\n",
    "            sent = []\n",
    "            for t in tokens:\n",
    "                if t.strip():\n",
    "                    sent.append(t.strip())\n",
    "            pos_sents.append(sent)\n",
    "\n",
    "    neg_sents = []\n",
    "    with open(neg_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokens = line.split(' ')\n",
    "            sent = []\n",
    "            for t in tokens:\n",
    "                if t.strip():\n",
    "                    sent.append(t.strip())\n",
    "            neg_sents.append(sent)\n",
    "\n",
    "    balance_len = min(len(pos_sents), len(neg_sents))\n",
    "\n",
    "    texts = pos_sents + neg_sents\n",
    "    labels = [1] * balance_len + [0] * balance_len\n",
    "\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer可以把原始文本转化为tf-idf的特征矩阵，从而为后续的文本相似度计算，主题模型(如LSI)，文本搜索排序等一系列应用奠定基础\n",
    "def KFold_validation(clf, X, y):\n",
    "    acc = []\n",
    "    pos_precision, pos_recall, pos_f1_score = [], [], []\n",
    "    neg_precision, neg_recall, neg_f1_score = [], [], []\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train, test in kf.split(X):\n",
    "        X_train = [X[i] for i in train]\n",
    "        X_test = [X[i] for i in test]\n",
    "        y_train = [y[i] for i in train]\n",
    "        y_test = [y[i] for i in test]\n",
    "\n",
    "        # vectorizer = TfidfVectorizer(analyzer='word', tokenizer=lambda x : (w for w in x.split(' ') if w.strip()))\n",
    "        def dummy_fun(doc):\n",
    "            return doc\n",
    "\n",
    "        vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                     tokenizer=dummy_fun,\n",
    "                                     preprocessor=dummy_fun,\n",
    "                                     token_pattern=None)\n",
    "\n",
    "        vectorizer.fit(X_train)\n",
    "        X_train = vectorizer.transform(X_train)\n",
    "        X_test = vectorizer.transform(X_test)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_test)\n",
    "\n",
    "        acc.append(metrics.accuracy_score(y_test, preds))\n",
    "        pos_precision.append(metrics.precision_score(y_test, preds, pos_label=1))\n",
    "        pos_recall.append(metrics.recall_score(y_test, preds, pos_label=1))\n",
    "        pos_f1_score.append(metrics.f1_score(y_test, preds, pos_label=1))\n",
    "        neg_precision.append(metrics.precision_score(y_test, preds, pos_label=0))\n",
    "        neg_recall.append(metrics.recall_score(y_test, preds, pos_label=0))\n",
    "        neg_f1_score.append(metrics.f1_score(y_test, preds, pos_label=0))\n",
    "\n",
    "\n",
    "    return (np.mean(acc), np.mean(pos_precision), np.mean(pos_recall), np.mean(pos_f1_score),\n",
    "            np.mean(neg_precision), np.mean(neg_recall), np.mean(neg_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_clfs():\n",
    "    print('Loading dataset...')\n",
    "\n",
    "    X, y = load_dataset_tokenized()\n",
    "\n",
    "    classifiers = [\n",
    "        ('LinearSVC', svm.LinearSVC()),\n",
    "        ('LogisticReg', LogisticRegression()),\n",
    "        ('SGD', SGDClassifier()),\n",
    "        ('MultinomialNB', naive_bayes.MultinomialNB()),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('DecisionTree', DecisionTreeClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier()),\n",
    "        ('AdaBoost', AdaBoostClassifier(base_estimator=LogisticRegression()))\n",
    "    ]\n",
    "\n",
    "    cols = ['metrics', 'accuracy',  'pos_precision', 'pos_recall', 'pos_f1_score', 'neg_precision', 'neg_recall', 'neg_f1_score']\n",
    "    scores = []\n",
    "    for name, clf in classifiers:\n",
    "        score = KFold_validation(clf, X, y)\n",
    "        row = [name]\n",
    "        row.extend(score)\n",
    "        scores.append(row)\n",
    "\n",
    "    df = pd.DataFrame(scores, columns=cols).T\n",
    "    df.columns = df.iloc[0]\n",
    "    df.drop(df.index[[0]], inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model():\n",
    "    print('Loading dataset...')\n",
    "\n",
    "    X, y = load_dataset_tokenized()\n",
    "\n",
    "    clf = svm.LinearSVC()\n",
    "\n",
    "    vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                 tokenizer=dummy_fun,\n",
    "                                 preprocessor=dummy_fun,\n",
    "                                 token_pattern=None)\n",
    "\n",
    "    X = vectorizer.fit_transform(X)\n",
    "\n",
    "    print('Train model...')\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    print('Loading comments...')\n",
    "    df = pd.read_csv(comment_file)\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['created_time'] = pd.to_datetime(df['created_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "    df['polarity'] = 0\n",
    "    df['title'].apply(lambda x: [w.strip() for w in x.split()])\n",
    "\n",
    "    texts = df['title']\n",
    "    texts = vectorizer.transform(texts)\n",
    "\n",
    "    preds = clf.predict(texts)\n",
    "    df['polarity'] = preds\n",
    "\n",
    "    df.to_csv('stock_comments_analyzed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "metrics        LinearSVC  LogisticReg       SGD  MultinomialNB       KNN  \\\n",
      "accuracy        0.881593     0.880834  0.882353       0.879640  0.820056   \n",
      "pos_precision   0.880544     0.879024  0.882389       0.882137  0.807142   \n",
      "pos_recall      0.882481     0.882941  0.881723       0.876015  0.840426   \n",
      "pos_f1_score    0.881483     0.880898  0.882029       0.879043  0.823396   \n",
      "neg_precision   0.882443     0.882578  0.882201       0.876666  0.833607   \n",
      "neg_recall      0.880121     0.878210  0.882241       0.883188  0.799070   \n",
      "neg_f1_score    0.881251     0.880306  0.882189       0.879890  0.815912   \n",
      "\n",
      "metrics        DecisionTree  RandomForest  AdaBoost  \n",
      "accuracy           0.795095      0.847514  0.771659  \n",
      "pos_precision      0.813365      0.869430  0.797129  \n",
      "pos_recall         0.765163      0.817591  0.798650  \n",
      "pos_f1_score       0.788482      0.842650  0.765202  \n",
      "neg_precision      0.778262      0.827652  0.825027  \n",
      "neg_recall         0.824773      0.877225  0.759168  \n",
      "neg_f1_score       0.800786      0.851654  0.766304  \n",
      "Loading dataset...\n",
      "Train model...\n",
      "Loading comments...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    scores = benchmark_clfs()\n",
    "    print(scores)\n",
    "    scores.to_csv('model_ml_scores.csv', float_format='%.4f')\n",
    "\n",
    "\n",
    "    eval_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
